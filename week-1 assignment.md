**1) Big Data with example and types**

Big Data is an enormous collection of data that is growing exponentially over time. No conventional data management tools can effectively store or process this data because of its size and complexity. 

**Examples** 

1) The New York stock Exchange data which produces one terabyte of data per day.
1) Social media produce more than 500 terabytes of new data everyday.The data is generated due to pics and videos uploading,messaging and writing comments.
1) In the course of a 30-minute flight, a single jet engine can produce 10 or more terabytes of data. With thousands of flights every day, the amount of data generated can reach many Petabytes.

**Types of Big data:**

**Structured data:**

Stuctured data is any information that can be processed, is readily available, and can be kept in a set format. Because it has highly coordinated measurements that are determined by setting parameters, structured data in big data is the most simple procedure to work with.

**Unstructured data:**

This kind of data is frequently challenging to analyze because it is not set up in a predetermined format. Emails, social media posts, and multimedia content like pictures and videos are a few examples.

**Semistructured data** :

This kind of data combines structured and unstructured data. Although it has some organizational structure, there are also some unstructured parts. Examples include log files, XML files, and certain kinds of emails.



**2) 6 ‘V’s of Big Data**

**Volume:**

Big data is the term used to describe enormous amounts of data that cannot be processed using a single machine. The volume of data being produced is increasing exponentially, necessitating new tools and methodologies are required for processing and analyzing such data.

**Velocity:**

the rate at which the data is created and gathered. As real-time data streams have grown, data is constantly being produced and upgraded, usually requiring real-time processing and analysis.

**Variety:**

Variety refers to the different forms of the data such as unstructured data(texts,images and videos),structured data(numbers and dates) and semi structured data(XML and JASON).

**Veracity:**Veracity is a term used to describe the accuracy and quality of data.The problem of veracity is brought on by the large amount of incomplete and inconsistent data.

**Variability:**

The variability of big data refers to its ability to be extremely dynamic and ever-changing. For this, we need tools and methods that can adapt to shifting data sources and pattern requirements.

**Value:**

Transforming data into value is referred to as value. Businesses may make money by transforming big data that has been accessed into values.

**3) Phases of Big Data analysis**

**Phase 1: Data Acquisition and Recording** 

Data is gathered from different sources.And most of the data is unstructured data.And this data is filtered by removing the corrupt or irrelevant data(which is of no use for analysis).And the filtered data is stored and compressed for future analysis. In order to describe the data being recorded and measured, automatic metadata creation is required.

**Phase 2 : Data Extractation and Cleaning**

The data that don't fit the analysis's underlying scope are extracted and transformed in this phase.And then the data cleaning is done.(Such as sorting the data,filling the missing values and identifying duplicates).

**Phase 3 : Aggregation and Representation**

In this phase the datasets are merged together to reduce and avoid redundancies and inconsistencies.And then the data is represented using many vizulaisation techniques such as boxplot,histogram,barchat e.t.c.

**Phase 4 : Data Modelling and Analysis**

Data sets are created for testing, training, and production during this phase .An abstract model known as a data model organizes data elements and standardizes how they relate to one another.And depending on the nature of the big data Confirmatory analysis and Exploratory analysis is done.

**Phase 5 : Interpretation** 

In this phase we understand and check whether the outcomes are  successful or not.And we need to identify the conclusions .And the model is ready for decision making.



**4) Challenges in Big Data analysis**

**Challenge 1: Heterogeneity and Incompleteness**

A machine analysis algorithm expects homogenous data, however, and cannot grasp any kind of differences or nuance. In addition, some errors and incompleteness remain after the data is cleaned. As a result, careful data structuring must be the first step.

**Challenge 2 : Scale**

Data management has been a challenging problem for a long time because of the rapid expansion of data. In the past, faster processors kept us from experiencing this problem by following Moore's law and giving us the resources required to handle increasing volumes of data. The current major shift is caused by the fact that CPU speeds are static while data volume is scaling faster than compute resources.

**Challenge 3 : Timeliness**

The size of the data set that needs to be processed directly affects how long it takes to analyze the data. The system design that addresses data size will also produce a system that can process a given size of data set more quickly.Multiple situations require for the analysis's findings to be available right away. So that a small amount of incremental computation with new data can be used to quickly arrive at a conclusion, we need to develop partial results in advance**.**

**Challenge 4 : Privacy**

Another major issue that is raised in the context of big data is data privacy. Because data from various sources are interconnected together, there is a lot of public concern about the improper use of personal information. To fully realize the potential of big data, handling privacy is a challenge that requires both technical and sociological solutions. Rethinking security for data sharing in Big Data use cases is a crucial step.

**Challenge 5 : Human Collabration**

Despite the enormous advancements made in computational analysis, there are still many patterns that humans can easily spot but computer algorithms have trouble finding. Analytics for Big Data should ideally not be entirely computational and should instead be explicitly designed to involve humans.



